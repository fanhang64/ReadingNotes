## Softmax回归

虽然叫回归，但是是一个分类问题。

让我们考虑一个简单的图像分类问题，其输入图像的高和宽均为2像素，且色彩为灰度。这样每个像素值都可以用一个标量表示。我 们将图像中的4像素分别记为 `x1,x2,x3,x4`。假设训练数据集中图像的真实标签为狗、猫或鸡（假设可以用4像素表示出这3种动物），这些标签分别对应离散值 `y1,y2,y3` 。
　　我们通常使用离散的数值来表示类别，例如 `y1=1,y2=2,y3=3`。如此，一张图像的标签为 1、2 和 3 这 3 个数值中的一个。虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到 1、2 和 3 这 3 个离散值之一，但这种连续值到离散值的转化通常会 影响到分类质量。因此我们一般使用更加适合离散值输出的模型来解决分类问题。

### Softmax回归模型

Softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，Softmax 回归的输出值个数等于标签里的类别数。因为一共有4种特征和3种输出动物类别，所以权重包含12个标量（带下标的 w）、偏差包含3个标量（带下标的 b），且对每个输入计算 `o1,o2,o3` 这 3 个输出：

<img src="https://gitee.com/fanhang64/my_images/raw/master/2021/image-20211024205352416.png" alt="image-20211024205352416" style="zoom: 50%;" />

Softmax 回归同线性回归一样，也是一个单层神经网络。由于每个输出 `o1,o2,o3`的计算都要依赖于所有的输入 `x1,x2,x3,x4`, Softmax回归的输出层也是一个全连接层。

<img src="https://gitee.com/fanhang64/my_images/raw/master/2021/image-20211024205833647.png" alt="image-20211024205833647" style="zoom:50%;" />



### Softmax 运算

这⾥要采取的主要⽅法是将模型的输出视作为概率。我们将优化参数以最⼤化观测数据的概率。为了得到
预测结果，我们将设置⼀个阈值，如选择具有最⼤概率的标签。我们希望模型的输出ŷ<sub>j</sub> 可以视为属于类`j `的概率。然后我们可以选择具有最⼤输出值的类别`argmax(j）`，`yj` 作为我们的预测。

既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值`oi` 当作预测类别是 `i` 的置信度, 并将值最大的输出所对应的类作为预测输出，即输出 `argmax(oi)` 。 例如，如果 `o1,o2,o3`分别为 `0.1,10,0.1 `, 由于`o2`最大, 那么预测类别为2，其代表猫。

然而，直接使用输出层的输出有两个问题。一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。例如，刚才举的例子中的输出值 10 表示图像类别为猫，因为该输出值是其他两类的输出值的 `100` 倍。但如果`o1`和`o3`的输出值为10<sup>3</sup>，则之前的输出值`10`却又表示图像类别为猫的概率很低。

另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。

Softmax运算符（softmax operator）解决了以上两个问题。它通过下式将输出值变换成值为正且和为 `1`的概率分布：

<img src="https://gitee.com/fanhang64/my_images/raw/master/2021/image-20211024215722999.png" alt="image-20211024215722999" style="zoom:50%;" />

其中，

<img src="https://gitee.com/fanhang64/my_images/raw/master/2021/image-20211024220132395.png" alt="image-20211024220132395" style="zoom:67%;" />

容易看出 `ŷ1+ŷ2+ŷ3=1`且 `0≤ŷ1,ŷ2,ŷ3≤1`，因此 `ŷ1,ŷ2,ŷ3`是一个合法的概率分布。

这个时候，如果`ŷ2=0.8`，则我们都知道图像类别为猫的概率为`80%`。

**注意：**

- softmax运算不改变预测类别输出。（argmax(oi) = argmax(ŷ)）



### 单样本分类的矢量计算表达式

　为了提高计算效率，我们可以将单样本分类通过矢量计算来表达。在上面的图像分类问题中，假设softmax回归的权重和偏差参数分别为

<img src="https://gitee.com/fanhang64/my_images/raw/master/2021/image-20211024222211333.png" alt="image-20211024222211333" style="zoom: 50%;" />

设高和宽分别为2个像素的图像样本`i`的特征为

<img src="https://gitee.com/fanhang64/my_images/raw/master/2021/image-20211024222243499.png" alt="image-20211024222243499" style="zoom:50%;" />

输出层的输出为

<img src="https://gitee.com/fanhang64/my_images/raw/master/2021/image-20211024222311342.png" alt="image-20211024222311342" style="zoom: 50%;" />

预测为狗、猫或鸡的概率分布为

<img src="https://gitee.com/fanhang64/my_images/raw/master/2021/image-20211024222344280.png" alt="image-20211024222344280" style="zoom:50%;" />

softmax回归对样本 `i` 分类的矢量计算表达式为

<img src="https://gitee.com/fanhang64/my_images/raw/master/2021/image-20211024222415286.png" alt="image-20211024222415286" style="zoom:50%;" />

### 小批量样本分类的矢量计算表达式

　　为了进一步提升计算效率，我们通常对小批量数据做矢量计算。广义上讲，给定一个小批量样本，其批量大小为 `n`，输入个数（特征数）为 `d` ，输出个数（类别数）为 `q`∘ 设批量特征为**X∈R<sup>n×d</sup> **。假设 softmax 回归的权重和偏差参数分别为 **W∈R<sup>d×q</sup>**和 **b∈R<sup>1×q</sup>**。 softmax 回归的矢量计算表达式为:

<img src="https://gitee.com/fanhang64/my_images/raw/master/2021/image-20211024224522236.png" alt="image-20211024224522236" style="zoom:50%;" />

其中的加法运算使用了广播机制，`O`，Y^∈R<sup>n×q</sup> 且这两个矩阵的第 `i` 行分别为样本 `i`的输出 `o(i)`和概率分布 ŷ<sup>(i)</sup>.



**分类 和 回归**

- **回归**估计一个连续值。
  - **单连续**数值的输出
  - 自然区间R
  - 跟真实值的区别作为损失
- **分类**预测一个**离散**类别。  
  - 通常多个输出
  - 输出`i`是预测为第`i`类的置信度

接下来我们看以下如何从回归过度到分类问题。

### 从回归到多类分类

首先对 **类别 ** 进行一位有效编码，假设我们有n个类别，则标号为**y**。假设我们真实的类别为第i个，则`yi`为1，其余元素全部为0。

<img src="https://i.loli.net/2021/10/22/ZspXz2KR1rSkJNW.png" alt="image-20211022155500455" style="zoom:67%;" />

然后可以使用**均方损失**训练，假设我们训练出一个模型，则训练时候，选取`i`，则最大化`Oi`的值为预测结果`ŷ`。

例如：标签`y`将是⼀个三维向量，其中(1; 0; 0)对应于"猫"、(0; 1; 0)对应于"鸡"、(0; 0; 1)对应于"狗"

### 交叉熵损失 (使用交叉熵作为softmax的loss函数)

1. 叉熵损失函数(CrossEntropy Loss)：分类问题中经常使用的一种损失函数

2. 交叉熵能够衡量同一个随机变量中的两个不同概率分布的差异程度，在机器学习中就表示为真实概率分布与预测概率分布之间的差异。**交叉熵的值越小，模型预测效果就越好**。

3. 交叉熵在分类问题中常常与softmax是标配，softmax将输出的结果进行处理，使其多个分类的预测值和为1，再**通过交叉熵来计算损失**。

交叉熵常用来衡量概率的区别。通过下面公式计算交叉熵：

![image-20211026111450793](https://i.loli.net/2021/10/26/d1tVnlKwvH489Q6.png)

将**交叉熵作为损失函数**，如下：

<img src="https://i.loli.net/2021/10/26/Qwqi6IxoNBYOgK1.png" alt="image-20211026112920693" style="zoom:67%;" />

**总结：**

- softmax 回归是一个多类分类模型
- 使用softmax操作子得到每个类的预测置信度
- 使用交叉熵来衡量预测和标号的区别



## 损失函数

我们需要⼀个损失函数来度量预测概率的效果。用来衡量预测值和真实值之间的区别。

### (1) 均方损失 L2 Loss

均方损失为：(1/2) *（预测值- 真实值）^2 

**公式：**   L(y, y') = (1 / 2) * ( y - y')<sup>2</sup> 



###  (2) 绝对值损失函数 L1 Loss 

绝对值损失函数： 预测值 - 真实值的绝对值。

**公式：** L(y, y') = |y - y'|



### (3) huber's Robust Loss

当预测值和真实值的绝对值差的大于1时候，为绝对值误差。

当预测值和真实值的比较接近时候，即绝对值小于1，为均方误差。

**公式：**

![image-20211022160615282](https://i.loli.net/2021/10/22/KCDHGWRp4btZSVY.png)



## 图像分类数据集（如何读取多类分类问题数据集）







## Softmax实现



##  Softmax简易实现

