

## 线性回归

### (1) 一个简化的模型

假设1：影响房价的关键因素是卧室数量，卫生间的个数，居住面积，记为x1，x2，x3

假设2：**成交价是关键因素的加权和** y = w1x1+w2x2+w3x3 + b

**权重**和**偏差**的实际值在后面决定。

### (2) 线性模型

给定n维的输入 **X** = [x1, x2, x3, ..., xn]<sup>T</sup>

线性模型有一个n维   **权重** **w** = [w1, w2, ..., wn]<sup>T</sup>，和一个标量   **偏差**b

**输出是输入的加权和**即y = w1x1 + w2x2 + ... + wnxn + b

**输出的向量版本**为：y = <**w, x**> + b

**线性模型可以看做为单层的神经网络。**



<img src="https://i.loli.net/2021/10/20/wpWLCojiaRNtexP.png" alt="image-20211018215919286" style="zoom: 67%;" />

### (3) 衡量预估质量

**比较真实值和预估值。**

假设 y 是真实值，ŷ 是估计值，则可以得出损失函数 **ζ(y, ŷ) = (1/ 2) * ( y - ŷ)<sup>2</sup>** ，这个叫做**平方损失**。

**值越小，越好。**

### (4) 训练数据

收集一些数据点，来决定参数值（权重和偏差），这些数据我们称之为**训练数据**。

通常越多越好。假设我们有n个样本，记

**X** = [**x1, x2, ... xn**]<sup>T</sup>      **y** = [y1, y2, ... , yn]<sup>T</sup>



### (5) 参数学习

#### 训练损失函数

根据我们之前的损失和给定我们数据，对于模型在每个数据上的损失，然后求均值，就会得到**损失函数**，写出来如下。（1/n 是n个值取平均）



![image-20211018222052337](https://i.loli.net/2021/10/20/NtwXvkKenqOSEma.png)



#### 最小化损失来学习参数

找个一个`w`和`b`使得损失函数的值最小。

![image-20211020211640774](https://i.loli.net/2021/10/20/8zHARq2W1vktCEx.png)

**总结：**

- 线性回归是对n维输入的加权，外加偏差。
- 线性回归通常使用**平方损失**来衡量预测值和真实值得到差异。
- 线性回归有显示解。
- 线性回归可以看成为单层的神经网络。

### (6) 基础优化算法



#### 梯度下降法

![image-20211020212426678](https://i.loli.net/2021/10/20/jLmvWMsTPYxHy31.png)



1. 挑选一个初始值**w0**。（随机取的地方）

2. 重复迭代参数t = 1, 2, 3。（例如参数w, 参数 偏差b)

   ![image-20211020212323299](https://i.loli.net/2021/10/20/2Xv9N8ZAUaFjgId.png)

**注意：**

- 沿梯度`（∂L /∂wt-1 ）`方向将增加损失函数值。（乘以负号， 则下降最快的方向。）
- 学习率 `η`： 步长的超参数。（人为指定的值，不能选太小，不能选太大）
- 随机梯度下降：随机是说，随机采样（每次取一个batch大小）。



#### 小批量随机梯度下降

由于计算机在整个训练集上计算梯度太贵（需要大量计算资源），导致一个深度神经网络模型可能需要数分钟至数小时。

**解决方法： ** （深度学习中一般使用 小批量随机梯度下降）

可以随机采样b个样本 i<sub>1</sub>，i<sub>2</sub>，...，i<sub>b</sub> 来计算近似的损失：

![image-20211020214107618](https://i.loli.net/2021/10/20/z8invYLKXFwftrG.png)

**注意：**

- **b为批量的大小**，是另一个重要的**超参数**。
- 批量大小也不能太大，批量太大内存消耗增加，浪费计算。
- 批量也不能太小，太小的话不适合并行计算来最大利用计算资源。



**总结：**

- 梯度下降是通过不断沿着反梯度方向更新参数来求解；
- 小批量随机梯度下降是深度学习默认的求解算法；
- 两个重要的超参数是**批量大小**和**学习率**。



**Pytorch中的梯度计算**

```python
x = torch.ones(2, requires_grad=True)
print(x)
# tensor([1., 1.], requires_grad=True)

z = x + 2
print(z)
# tensor([3., 3.], grad_fn=<AddBackward0>)

z.backward()  # 来自动计算所有的梯度
print(x.grad)

此时报错 RuntimeError: grad can be implicitly created only for scalar outputs   只能是标量
```

![image-20211021214049190](https://i.loli.net/2021/10/21/2u9RWEUqoermHjh.png)

**解决方法：**

 那么我们只要想办法把矩阵转变成一个标量不就好了？比如我们可以对z求和，然后用求和得到的标量在对x求导，这样不会对结果有影响，例如：

```python
x = torch.ones(2, requires_grad=True)
print(x)
# tensor([1., 1.], requires_grad=True)

z = x + 2
print(z)
# tensor([3., 3.], grad_fn=<AddBackward0>)

z.sum().backward()  # 来自动计算所有的梯度
print(z)
# tensor([3., 3.], grad_fn=<AddBackward0>)

print(x.grad)
#  tensor([1., 1.])
```

![image-20211021214326209](https://i.loli.net/2021/10/21/URDexbVtoz5IM9d.png)

## 线性回归的实现

```python
%matplotlib inline

import random
import torch
from d2l import torch as d2l

"""
根据带有噪声的线性模型构造一个人造数据集。使用线性模型参数w = [2, -3, 4]T、b = 4.2 和噪声项ε生成数据集及其标签：
w 为权重； b 为偏差
y = Xw + b +ε
"""
def sysnthetic_data(w, b, num_examples):
    """生成 y = Xw + b + 噪声"""
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b  # matmul 矩阵相乘（行与列元素，相乘相加）
    y += torch.normal(0, 0.01, y.shape)  # 均值为0，方差为0.01的噪声
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = sysnthetic_data(true_w, true_b, 10000)


"""
features 中的每一行都包含一个二维数据样本，lables中的每一行都含有一维标签值（一个标量)
"""
print('features:', features[0], '\nlabels：', labels[0])
d2l.set_figsize()
# 需要执行detach()才能转numpy
d2l.plt.scatter(features[:, 1].detach().numpy(), labels.detach().numpy(), 1)


"""
data_iter函数，该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为batch_size的小批量
"""
def data_iter(batch_size, features, labels):
    num_examples = len(features)  # 样本个数, 获取第一维个数
    print(num_examples)
    indices = list(range(num_examples))  # [0, ... , 9999]
    random.shuffle(indices)  # 将样本打乱，进行随机读取
    
    for i in range(0, num_examples, batch_size):   # 生成器 for 循环上面的部分只执行一次
        batch_indices = torch.tensor(
            indices[i:min(i+batch_size, num_examples)]
        )
        yield features[batch_indices], labels[batch_indices]

batch_size = 10

for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break


    
"""
定义初始化模型参数  && 定义模型
"""
w = torch.normal(0, 0.1, size=(2, 1), requires_grad=True) # requires_grad 计算梯度

b = torch.zeros(1, requires_grad=True)  # b为偏差

print(w, b)


"""
定义线性回归模型
"""
def line_regression(X, w, b):
    """线性回归模型"""
    return torch.matmul(X, w) + b



"""
定义损失函数
"""
def squared_loss(y_hat, y):
    """均方损失"""
    return (1/2) * ((y_hat - y) ** 2)


"""
小批量随机梯度下降
"""
def sgd(params:list, lr, batch_size):  # parmas 模型参数，lr 学习率， 
    """小批量随机梯度下降"""

    with torch.no_grad():  # 不需要进行梯度
        for param in params:
            param -= (lr * param.grad / batch_size)  #  lr * param.grad 为学习率乘以梯度；除batch_size 是损失函数取均值
            param.grad.zero_()  # 手动的将梯度设置为0，用于下次循环的计算

            
"""
训练过程
"""
lr = 0.03
num_epochs = 3  # 数据扫3遍
net = line_regression  # 线性回归模型

loss = squared_loss  # 损失函数


for epoch in range(num_epochs): 
    for X, y in data_iter(batch_size, features, labels):  # 每次取batch_size的X和y
        y_hat = net(X, w, b)
        l = loss(y_hat, y)
        l.sum().backward()  # 反向传播，该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数

        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新w和b

    with torch.no_grad():
        train_1 = loss(net(features, w, b), labels)  # 整个数据feature和labels计算损失函数
        print(f'epochs {epoch + 1}, loss {float(train_1.mean()):f}')  # float输出

```



## 线性回归的简洁实现

##### 方法：torch.utils.data.dataset.TensorDataset

- 对给定的`tensor`数据(样本和标签)，将它们包装成`dataset`。注意，如果是`numpy`的`array`，或者`Pandas`的`DataFrame`需要先转换成`Tensor`。   可以按照索引值，进行取值。

  ```python
  import torch.utils.data
  
  # 将训练数据的特征和标签组合
  dataset = data.TensorDataset(*[features, labels])
  
  print(dataset[0])
  # (tensor([ 0.8367, -2.1969]), tensor([13.3321]))
  ```



##### 方法：数据加载器 torch.utils.data.dataset.DataLoader

- 数据加载器，组合数据集和采样器，并在数据集上提供单进程或多进程迭代器。它可以对我们上面所说的数据集`Dataset`作进一步的设置。

  ```python
  dataset (Dataset) – 加载数据的数据集。
  
  batch_size (int, optional) – 每个batch加载多少个样本(默认: 1)。
  
  shuffle (bool, optional) – 设置为True时会在每个epoch重新打乱数据(默
  认: False).
  
  sampler (Sampler, optional) – 定义从数据集中提取样本的策略。如果指
  定，则shuffle必须设置成False。
  
  num_workers (int, optional) – 用多少个子进程加载数据。0表示数据将在
  主进程中加载(默认: 0)
  
  pin_memory：内存寄存，默认为False。在数据返回前，是否将数据复制到
  CUDA内存中。
  
  drop_last (bool, optional) – 如果数据集大小不能被batch size整除，
  则设置为True后可删除最后一个不完整的batch。如果设为False并且数据集的
  大小不能被batch size整除，则最后一个batch将更小。(默认: False)
  
  timeout：是用来设置数据读取的超时时间的，如果超过这个时间还没读取
  到数据的话就会报错。 所以，数值必须大于等于0。
  ```

  

**实现**

```python
import numpy as np
import torch

from torch.utils import data
from d2l import torch as d2l 

true_w = torch.tensor([2., -3.4])
true_b = 4.2

features, labels = d2l.synthetic_data(true_w, true_b, num_examples=10000)
print(features, labels)



"""
使用框架的API读取数据
"""
def load_array(data_arrays, batch_size, is_train=True):
    """构造一个Pytorch 数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)  # 参数为 多个tensor
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)  # 返回可迭代对象


next(iter(data_iter))



"""
模型定义
"""
from torch import nn  # nn是神经网络的缩写

net = nn.Sequential(nn.Linear(2, 1))  # 把模型放到Sequential的容器中


"""
初始化w和b, 用于梯度计算
"""
net[0].weight.data.normal_(0, 0.01)  # 设置w。  net[0].weight 为用于梯度计算的w
net[0].bias.data.fill_(0)  #  填充b为0。 net[0].bias 填充0



"""
计算均方误差，这里使用MSELoss类
"""
loss = nn.MSELoss()


"""
实例化随机梯度下降SGD实例
"""
trainer = torch.optim.SGD(net.parameters(), lr=0.03)


"""
训练过程
"""
num_epochs = 3

for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X), y)
        trainer.zero_grad()  # 所有参数的梯度清零。
        l.backward()  # 反向传播， 计算梯度
        trainer.step()  # 更新所有参数
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')

```





