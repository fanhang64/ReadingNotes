## 多层感知机

相比线性回归和softmax回归等单层神经网络，深度学习更关注多层模型。

**感知机问题：**

- 感知机不能拟合XOR函数，只能产生线性分割面。  （多层感知机可以做）

### 隐藏层

**多层感知机在单层神经网络上引入隐藏层**。隐藏层位于输入层和输出层之间。

<img src="https://i.loli.net/2021/11/07/u1C6wl3Uo4DghIF.png" alt="image-20211107091639406" style="zoom:67%;" />

在上图所示的多层感知机中，输入和输出的个数分别为4和3，中间隐藏层含有5个隐藏单元，由于输入层不涉及计算，多层感知机的层数为2。隐藏层中的神经元和输入层的各个输入完全连接，输出层中的神经元和隐藏层的各个神经元也完全连接。因此多层感知机中的隐藏层和输出层都是全连接层。



给定小批量样本**X**，其批量大小为n，输入个数为d。假设多层感知机只含有一个隐藏层，其中隐藏单元个数为h，记隐藏层的输出为**H**，

因为隐藏层和输出层均为全连接层，可以设置隐藏层的权重参数和偏差参数分别为W<sub>h</sub>和b<sub>h</sub>，对应输出层的权重和偏差参数为W<sub>o</sub>和b<sub>o</sub>

单隐藏层的多层感知机输出**O**公式为：

![image-20211107094329182](https://i.loli.net/2021/11/07/YhzngmZNeF5E6Sp.png)

如果将隐藏层的输出作为输出层的输入，可以得到：

<img src="https://i.loli.net/2021/11/07/AtVFLIrTQqfeGHC.png" alt="image-20211107094402281" style="zoom:80%;" />

可以看出来，即便添加更多隐藏层，上述设计**依然是单层的神经网络**，其中输出层权重参数为W<sub>h</sub>W<sub>o</sub>，偏差参数为b<sub>h</sub>W<sub>o</sub>+b<sub>o</sub>



### 多隐藏层

![image-20211113204609316](https://i.loli.net/2021/11/13/ZEXHjLY76Q3hbOF.png)



**注意：**

- 隐藏层的大小是**超参数**



### 激活函数

上述问题的根源在于，全连接层只是对数据做仿射变化，多个仿射变换的叠加仍然是一个仿射变换。

解决办法：是引入非线性变换，有了激活函数，就不可能再将我们的多层感知机退化成线性模型。例如：对隐藏层变量使用按元素运算的非线性变换，然后在作为下一个连接层的输入。这个非线性函数称为**激活函数**。下面是几种常见的激活函数。



#### **通用近似定理**

多层感知机可以通过隐藏神经元捕捉到我们输入之间复杂的相互作用，这些神经元依赖每个输入的值。例如：在一对输入的上进行基本逻辑操作，多层感知机是通用近似器。即时网络只有一个隐藏层，给定足够的神经元和正确的权重，我们仍然可以对任意函数建模，尽管实际中学习该函数是很困难的。虽然一层的神经网络能够学习任何函数，但并不意味着应该尝试使用单隐藏层网络来解决所有问题。 事实上，**通过使用更深（不是更广）的网络，我们可以更容易地逼近许多函数。**

#### （1）ReLU函数

**ReLU**函数提供一个简单的线性变换，给定元素x，该函数的定义为

![image-20211107095239782](https://i.loli.net/2021/11/07/RSuFb4hcaXWLqot.png)

ReLU函数只保留正数元素，并将负数元素清零。（ReLU函数通过将相应的激活值设为0来仅保留正元素并丢弃所有负元素。）

当输入为负时，ReLU函数的值都为0，对0求导导数为0，而当输入为正时，ReLU函数为x，对x求导，则导数为1。但是当输入值为0时候，此时不可导，默认使用左侧导数。

```python
"""
   绘制ReLU函数
"""
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.relu(x)

d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))


"""
绘制ReLU的导数图像
"""
y.backward(torch.ones_like(x), retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))

```

**总结：**

使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题



#### （2）sigmoid函数

sigmoid 函数将输入变换为区间(0, 1)上的输出。因此，sigmoid函数常被称为挤压函数，它将范围在（-inf, inf)中的任意输入压缩到(0, 1)中的某个值。

![image-20211110224315200](https://i.loli.net/2021/11/10/sqRf87LJT2WNgdh.png)

sigmoid函数是一个平滑的可微的阈值单元近似，当我们想要将输出视为二分类问题的概率时，sigmoid仍然被广泛用作输出单元上的激活函数。然而，sigmoid在隐藏层中已经较少使用，被ReLU替代。

对sigmoid函数求导： 

<img src="https://i.loli.net/2021/11/10/EakgKJuf8USwNO6.png" alt="image-20211110230405395" style="zoom:80%;" />

```python
"""sigmoid函数绘制
"""
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = torch.sigmoid(x)

d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))

"""sigmoid导数图像绘制
"""
y.backward(torch.ones_like(x), retain_graph=True)
d2l.plot(x.detach(), x.grad, 'x', 'grad of sogmoid', figsize=(5, 2.5))
```

**总结：**

注意，sigmoid的导函数，当输入为0时，sigmoid函数的导数达到最大值0.25。而输入在任一方向上越远离0点，导数越接近0。

#### （3） tanh函数

**tanh(双曲正切)函数**能将其输入压缩转换到区间(-1, 1)上。

公式如下：

![image-20211110230900423](https://i.loli.net/2021/11/10/ZSkq24JgbF3pzCo.png)

**注意：**

- tanh函数，当输入在0附近时，tanh函数接近线性变换。函数的形状类似于sigmoid函数，不同的是**tanh函数关于坐标系原点中心对称**。

对tanh函数求导，导函数如下：

![image-20211110231215920](https://i.loli.net/2021/11/10/c1fFJTkIPY9O8Xr.png)

**总结：**

- 多层感知机**在输出层和输入层之间增加一个或多个全连接的隐藏层，并通过激活函数转换隐藏层的输出**。
- 常用的激活函数包括ReLU函数、sigmoid函数和tanh函数。
- 使用softmax来处理多分类
- **隐藏层数 和 各个隐藏层大小**为超参数



### 多层感知机实现

**从零开始实现**

```python
import torch
from torch import nn

from d2l import torch as d2l


# 初始化参数
num_inputs, num_outputs, num_hiddens = 784, 10, 256  # 超参数：隐藏层含有的隐藏单元个数num_hiddens = 256，选的784和10之间的数

W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad=True) *0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad=True))

W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad=True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad=True))

params = [W1, b1, W2, b2]


# 定义激活函数
def relu(X):
    y = torch.zeros_like(X)
    return torch.max(X, y)


# 定义模型
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X.matmul(W1)+b1)
    return H@W2 + b2  # @ == mulmat 叉乘。 注意：mul 和mul_ ，带下划线是默认in-place=True


# 定义损失函数
loss = nn.CrossEntropyLoss()

# 模型训练
num_epochs, lr = 10, 0.1

updater = torch.optim.SGD(params, lr=lr)

d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)


# 模型测试
d2l.predict_ch3(net, test_iter)
```